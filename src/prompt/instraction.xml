<instructions>
  <workflow_overview>
    Based on the provided requirements, generate a Dify workflow file (YAML format) containing the following elements:
    - Workflow composed of nodes (start, LLM, end)
    - Nodes are connected for sequential execution (parallel execution possible if specified)
    - Automates a series of processes from input to output
  </workflow_overview>

  <required_structure>
    <app_info>
      - mode: Specify workflow
      - name: Set a name representing the purpose
      - version: Use 0.1.5
    </app_info>
    <workflow_graph>
      <edges>
        - source: Start node ID
          target: LLM node ID
          data:
            sourceType: start
            targetType: llm
        - source: LLM node ID
          target: End node ID
          data:
            sourceType: llm
            targetType: end
      </edges>

      <nodes>
        <start_node>
          - id: Unique ID
          - type: start
          - variables:
            # File input example
            - type: file
              variable: input_document
              label: Document
              required: true
              max_length: 48
              allowed_file_types:
                - document
              allowed_file_upload_methods:
                - local_file
                - remote_url

            # Number input example
            - type: number
              variable: input_number
              label: Number
              required: true
              min: 0
              max: 1000000

            # Paragraph input example
            - type: paragraph
              variable: paragraph
              label: Paragraph
              required: true
              max_length: 48
              options: []

            # Short text input example
            - type: text-input
              variable: short_text
              label: Short Text
              required: true
              max_length: 48
              options: []
        </start_node>
        <llm_node>
          - id: Unique ID
          - type: llm
          - model:
            - provider: openai
            - name: gpt-4o
            - mode: chat
            - completion_params:
              - temperature: Generation temperature setting
          - prompt_template:
            - id: Unique ID (e.g. 'prompt1')
              role: system
              text: Prompt text enclosed in single quotes
          - context:
            - enabled: true
            - variable_selector: Specify variables to use
          - vision:
            - enabled: [true/false]
        </llm_node>
        <end_node>
          - id: Unique ID
          - type: end
          - outputs: # Output variable definitions
            - input_data: Input data
            - generated_text: Generated text
          </end_node>
  </required_structure>

  <usage_guide> 
    <notice>
    - Node IDs must be unique
    </notice>
    <how_to_use>
      <start_node>
        <use_case>
          - Functions as workflow starting point
          - Accepts user input
          - Stores input information as variables
        </use_case>
        <structure>
          - id: Unique ID (required)
          - type: start (fixed)
          - variables: Array of input variables (required)
          - type: Variable type (required)
          - variable: Variable name (required)
          - label: Display label (required)
          - required: Whether input is mandatory (required)
          - Other settings (depending on type)
        </structure>
        <variable_type>
          - file (file input):
            - allowed_file_types: Allowed file types
            - document: Document files
            - image: Image files
            - audio: Audio files
            - video: Video files
          - allowed_file_upload_methods: Upload methods
            - local_file: Local file
            - remote_url: Remote URL
          - max_length: Maximum filename length
          - required: Whether field is mandatory
        - number (number input):
          - min: Minimum value (optional)
          - max: Maximum value (optional)
        - paragraph (paragraph text):
          - max_length: Maximum character count (optional)
          - options: Options (optional)
        - text-input (short text):
          - max_length: Maximum character count (optional)
            - options: Options (optional)
        </variable_type>
      </start_node>
      <llm_node>
        <use_case>
          - Text generation and response creation
          - Input text processing and transformation
          - Answer generation for questions
        </use_case>
        <structure>
          - id: Unique ID (required)
          - type: llm (fixed)
          - model: (required)
            - provider: openai (fixed)
            - name: gpt-4o-mini (fixed)
            - mode: chat (fixed)
            - completion_params:
              - temperature: Value between 0.0-1.0 (required)
          - prompt_template: (required)
            - role: system (fixed)
            - text: Prompt text (required)
          - context: (required)
            - enabled: true (fixed)
            - variable_selector: Specify variables to use (required)
          - vision:
            - enabled: true
        </structure>
        <note>
          - Prompt must be enclosed in single quotes
          - Only system role prompts can be used
          - Variable reference format: {{#nodeID.variableName#}}
          - Variables specified in context's variable_selector should also be used in prompt_template's text
        </note>
      </llm_node>
      <end_node>
        <use_case>
          - Functions as workflow endpoint
          - Defines output of processing results
          - Data handoff to subsequent systems
        </use_case>
        <structure>
          - id: Unique ID (required)
          - type: end (fixed)
          - outputs: (required)
          - value_selector: (required)
            - [nodeID]
            - [variableName]
          - variable: Output variable name (required)
        </structure>
        <note>
          - At least one output variable required
          - Multiple output variables can be defined
        </note>
      </end_node>
    </how_to_use>
  </usage_guide>


  <output_format>
    The generated YAML file should follow this format:
    ```yaml
    app:
      mode: workflow
      name: [workflow name]
      version: 0.1.5

    workflow:
      graph:
        edges:
          # IF/ELSE branch edge example
          - source: [IF/ELSE node ID]
            target: [target node ID]
            data:
              sourceType: if-else
              targetType: [target node type]
            sourceHandle: 'true'  # When IF condition is met
          - source: [IF/ELSE node ID]
            target: [another target node ID]
            data:
              sourceType: if-else
              targetType: [target node type]
            sourceHandle: 'false'  # ELSE condition

        nodes:
          - id: [start node ID]
            data:
              type: start
              title: Start
              variables:
                # File input example
                - type: file
                  variable: input_document
                  label: Document
                  required: true
                  max_length: 48
                  allowed_file_types:
                    - document
                  allowed_file_upload_methods:
                    - local_file
                    - remote_url

                # Other input types
                - type: text-input
                  variable: [variable name]
                  type: string/number
                  label: [input field label]
                  required: true
                  max_length: [max character count]

          - id: [LLM node ID]
            data:
              type: llm
              title: LLM
              model:
                provider: openai
                name: gpt-4o-mini
                mode: chat
                completion_params:
                  temperature: 0.7
              prompt_template:
                - id: [prompt ID]
                  role: system
                  text: '[prompt text]'
              context:
                enabled: true
                variable_selector:
                  - [start node ID]
                  - [variable name]
              vision:
                enabled: [true/false]
          - id: [end node ID]
            data:
              type: end
              title: End
              outputs:
                - value_selector:
                    - [start node ID]
                    - [variable name]
                  variable: inputData
                - value_selector:
                    - [LLM node ID]
                    - text
                  variable: generatedText

      ```
  </output_format>
</instructions>
