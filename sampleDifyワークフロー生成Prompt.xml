<metadata>
  <title> Difyワークフロー生成プロンプト</title>
  <version> 2.0.0</version>
  <description>: Difyのワークフローファイルを生成するためのプロンプト定義</description>
</metadata>

<node_dsl_sample>
    <start node dsl>
workflow:
  graph:
    nodes:
    - data: # 例1
        desc: ''
        selected: false
        title: 開始
        type: start
        variables:
        - label: input
          max_length: 99999
          options: []
          required: true
          type: paragraph
          variable: input
      id: '1736385488409'
    - data: # 例2
        desc: ''
        selected: false
        title: 開始
        type: start
        variables:
        - allowed_file_extensions: []
          allowed_file_types:
          - image
          - document
          allowed_file_upload_methods:
          - local_file
          label: image_or_text
          max_length: 48
          options: []
          required: true
          type: file
          variable: image_or_text
      id: '1736386006466'
    </start node dsl>
    <llm node dsl>
workflow:
  graph:
    nodes:
    - data: #textの例
        context:
          enabled: true
          variable_selector:
          - '1736385488409'
          - input
        desc: ''
        model:
          completion_params:
            temperature: 0.7
          mode: chat
          name: o1-mini
          provider: openai
        prompt_template:
        - id: 65c46dbe-98b4-4a3f-aafb-16f668d4ee86
          role: system
          text: '{{#1736385488409.input#}}を要約してSNSで万バズを稼げる投稿文を生成'
        selected: false
        title: LLM
        type: llm
        variables: []
        vision:
          enabled: false
      id: '1736385490749'
    - data: # visionの例
        context:
          enabled: false
          variable_selector: []
        desc: ''
        model:
          completion_params:
            temperature: 0.7
          mode: chat
          name: gpt-4o
          provider: openai
        prompt_template:
        - id: b054d56c-790a-4076-ac71-af6e82378d97
          role: system
          text: 添付画像には何が写っているか日本語で説明せよ
        selected: false
        title: vision LLM
        type: llm
        variables: []
        vision:
          configs:
            detail: low
            variable_selector:
            - '1736386006466'
            - image
          enabled: true
      id: '1736386321565'
    </llm node dsl>
    <end node dsl>
workflow:
  graph:
    nodes:
    - data: # 例1
        desc: ''
        outputs:
        - value_selector:
          - '1736385490749'
          - text
          variable: text
        selected: true
        title: 終了
        type: end
      id: '1736385492836'
    - data: # 例2
        desc: ''
        outputs:
        - value_selector:
          - '1736386321565'
          - text
          variable: text
        - value_selector:
          - '1736386369722'
          - text
          variable: text
        - value_selector: []
          variable: ''
        selected: true
        title: 終了
        type: end
      id: '1736386425450'
    </end node dsl>
</node_dsl_sample>
<edge_dsl_sample>
    <start node dsl>
workflow:
  graph:
    edges:
    - data:
        isInIteration: false
        sourceType: start
        targetType: llm
      id: 1736385488409-source-1736385490749-target
      source: '1736385488409'
      sourceHandle: source
      target: '1736385490749'
      targetHandle: target
    - data:
        isInIteration: false
        sourceType: llm
        targetType: end
      id: 1736385490749-source-1736385492836-target
      source: '1736385490749'
      sourceHandle: source
      target: '1736385492836'
      targetHandle: target
    </start node dsl>
    <llm node dsl>
workflow:
  graph:
    edges:
    - data:
        isInIteration: false
        sourceType: start
        targetType: llm
      id: 1736385488409-source-1736385490749-target
      source: '1736385488409'
      sourceHandle: source
      target: '1736385490749'
      targetHandle: target
    - data:
        isInIteration: false
        sourceType: llm
        targetType: end
      id: 1736385490749-source-1736385492836-target
      source: '1736385490749'
      sourceHandle: source
      target: '1736385492836'
      targetHandle: target
    </llm node dsl>
    <end node dsl>
workflow:
  graph:
    edges:
    - data:
        isInIteration: false
        sourceType: start
        targetType: llm
      id: 1736385488409-source-1736385490749-target
      source: '1736385488409'
      sourceHandle: source
      target: '1736385490749'
      targetHandle: target
    - data:
        isInIteration: false
        sourceType: llm
        targetType: end
      id: 1736385490749-source-1736385492836-target
      source: '1736385490749'
      sourceHandle: source
      target: '1736385492836'
      targetHandle: target
    </end node dsl>
</edge_dsl_sample>

<source_code>

    <core.workflow.nodes.start.entities>
    from collections.abc import Sequence

    from pydantic import Field

    from core.app.app_config.entities import VariableEntity
    from core.workflow.nodes.base import BaseNodeData


    class StartNodeData(BaseNodeData):
        """
        Start Node Data
        """

        variables: Sequence[VariableEntity] = Field(default_factory=list)

    </core.workflow.nodes.start.entities>
    <core.app.app_config.entities>
    from collections.abc import Sequence
    from enum import Enum, StrEnum
    from typing import Any, Optional

    from pydantic import BaseModel, Field, field_validator

    from core.file import FileTransferMethod, FileType, FileUploadConfig
    from core.model_runtime.entities.message_entities import PromptMessageRole
    from models.model import AppMode


    class ModelConfigEntity(BaseModel):
        """
        Model Config Entity.
        """

        provider: str
        model: str
        mode: Optional[str] = None
        parameters: dict[str, Any] = {}
        stop: list[str] = []


    class AdvancedChatMessageEntity(BaseModel):
        """
        Advanced Chat Message Entity.
        """

        text: str
        role: PromptMessageRole


    class AdvancedChatPromptTemplateEntity(BaseModel):
        """
        Advanced Chat Prompt Template Entity.
        """

        messages: list[AdvancedChatMessageEntity]


    class AdvancedCompletionPromptTemplateEntity(BaseModel):
        """
        Advanced Completion Prompt Template Entity.
        """

        class RolePrefixEntity(BaseModel):
            """
            Role Prefix Entity.
            """

            user: str
            assistant: str

        prompt: str
        role_prefix: Optional[RolePrefixEntity] = None


    class PromptTemplateEntity(BaseModel):
        """
        Prompt Template Entity.
        """

        class PromptType(Enum):
            """
            Prompt Type.
            'simple', 'advanced'
            """

            SIMPLE = "simple"
            ADVANCED = "advanced"

            @classmethod
            def value_of(cls, value: str):
                """
                Get value of given mode.

                :param value: mode value
                :return: mode
                """
                for mode in cls:
                    if mode.value == value:
                        return mode
                raise ValueError(f"invalid prompt type value {value}")

        prompt_type: PromptType
        simple_prompt_template: Optional[str] = None
        advanced_chat_prompt_template: Optional[AdvancedChatPromptTemplateEntity] = None
        advanced_completion_prompt_template: Optional[AdvancedCompletionPromptTemplateEntity] = None


    class VariableEntityType(StrEnum):
        TEXT_INPUT = "text-input"
        SELECT = "select"
        PARAGRAPH = "paragraph"
        NUMBER = "number"
        EXTERNAL_DATA_TOOL = "external_data_tool"
        FILE = "file"
        FILE_LIST = "file-list"


    class VariableEntity(BaseModel):
        """
        Variable Entity.
        """

        variable: str
        label: str
        description: str = ""
        type: VariableEntityType
        required: bool = False
        max_length: Optional[int] = None
        options: Sequence[str] = Field(default_factory=list)
        allowed_file_types: Sequence[FileType] = Field(default_factory=list)
        allowed_file_extensions: Sequence[str] = Field(default_factory=list)
        allowed_file_upload_methods: Sequence[FileTransferMethod] = Field(default_factory=list)

        @field_validator("description", mode="before")
        @classmethod
        def convert_none_description(cls, v: Any) -> str:
            return v or ""

        @field_validator("options", mode="before")
        @classmethod
        def convert_none_options(cls, v: Any) -> Sequence[str]:
            return v or []


    class ExternalDataVariableEntity(BaseModel):
        """
        External Data Variable Entity.
        """

        variable: str
        type: str
        config: dict[str, Any] = {}


    class DatasetRetrieveConfigEntity(BaseModel):
        """
        Dataset Retrieve Config Entity.
        """

        class RetrieveStrategy(Enum):
            """
            Dataset Retrieve Strategy.
            'single' or 'multiple'
            """

            SINGLE = "single"
            MULTIPLE = "multiple"

            @classmethod
            def value_of(cls, value: str):
                """
                Get value of given mode.

                :param value: mode value
                :return: mode
                """
                for mode in cls:
                    if mode.value == value:
                        return mode
                raise ValueError(f"invalid retrieve strategy value {value}")

        query_variable: Optional[str] = None  # Only when app mode is completion

        retrieve_strategy: RetrieveStrategy
        top_k: Optional[int] = None
        score_threshold: Optional[float] = 0.0
        rerank_mode: Optional[str] = "reranking_model"
        reranking_model: Optional[dict] = None
        weights: Optional[dict] = None
        reranking_enabled: Optional[bool] = True


    class DatasetEntity(BaseModel):
        """
        Dataset Config Entity.
        """

        dataset_ids: list[str]
        retrieve_config: DatasetRetrieveConfigEntity


    class SensitiveWordAvoidanceEntity(BaseModel):
        """
        Sensitive Word Avoidance Entity.
        """

        type: str
        config: dict[str, Any] = {}


    class TextToSpeechEntity(BaseModel):
        """
        Sensitive Word Avoidance Entity.
        """

        enabled: bool
        voice: Optional[str] = None
        language: Optional[str] = None


    class TracingConfigEntity(BaseModel):
        """
        Tracing Config Entity.
        """

        enabled: bool
        tracing_provider: str


    class AppAdditionalFeatures(BaseModel):
        file_upload: Optional[FileUploadConfig] = None
        opening_statement: Optional[str] = None
        suggested_questions: list[str] = []
        suggested_questions_after_answer: bool = False
        show_retrieve_source: bool = False
        more_like_this: bool = False
        speech_to_text: bool = False
        text_to_speech: Optional[TextToSpeechEntity] = None
        trace_config: Optional[TracingConfigEntity] = None


    class AppConfig(BaseModel):
        """
        Application Config Entity.
        """

        tenant_id: str
        app_id: str
        app_mode: AppMode
        additional_features: AppAdditionalFeatures
        variables: list[VariableEntity] = []
        sensitive_word_avoidance: Optional[SensitiveWordAvoidanceEntity] = None


    class EasyUIBasedAppModelConfigFrom(Enum):
        """
        App Model Config From.
        """

        ARGS = "args"
        APP_LATEST_CONFIG = "app-latest-config"
        CONVERSATION_SPECIFIC_CONFIG = "conversation-specific-config"


    class EasyUIBasedAppConfig(AppConfig):
        """
        Easy UI Based App Config Entity.
        """

        app_model_config_from: EasyUIBasedAppModelConfigFrom
        app_model_config_id: str
        app_model_config_dict: dict
        model: ModelConfigEntity
        prompt_template: PromptTemplateEntity
        dataset: Optional[DatasetEntity] = None
        external_data_variables: list[ExternalDataVariableEntity] = []


    class WorkflowUIBasedAppConfig(AppConfig):
        """
        Workflow UI Based App Config Entity.
        """

        workflow_id: str

    </core.app.app_config.entities>

    <core.workflow.nodes.llm.entities>
    from collections.abc import Sequence
    from typing import Any, Optional

    from pydantic import BaseModel, Field, field_validator

    from core.model_runtime.entities import ImagePromptMessageContent
    from core.prompt.entities.advanced_prompt_entities import ChatModelMessage, CompletionModelPromptTemplate, MemoryConfig
    from core.workflow.entities.variable_entities import VariableSelector
    from core.workflow.nodes.base import BaseNodeData


    class ModelConfig(BaseModel):
        provider: str
        name: str
        mode: str
        completion_params: dict[str, Any] = {}


    class ContextConfig(BaseModel):
        enabled: bool
        variable_selector: Optional[list[str]] = None


    class VisionConfigOptions(BaseModel):
        variable_selector: Sequence[str] = Field(default_factory=lambda: ["sys", "files"])
        detail: ImagePromptMessageContent.DETAIL = ImagePromptMessageContent.DETAIL.HIGH


    class VisionConfig(BaseModel):
        enabled: bool = False
        configs: VisionConfigOptions = Field(default_factory=VisionConfigOptions)

        @field_validator("configs", mode="before")
        @classmethod
        def convert_none_configs(cls, v: Any):
            if v is None:
                return VisionConfigOptions()
            return v


    class PromptConfig(BaseModel):
        jinja2_variables: Sequence[VariableSelector] = Field(default_factory=list)

        @field_validator("jinja2_variables", mode="before")
        @classmethod
        def convert_none_jinja2_variables(cls, v: Any):
            if v is None:
                return []
            return v


    class LLMNodeChatModelMessage(ChatModelMessage):
        text: str = ""
        jinja2_text: Optional[str] = None


    class LLMNodeCompletionModelPromptTemplate(CompletionModelPromptTemplate):
        jinja2_text: Optional[str] = None


    class LLMNodeData(BaseNodeData):
        model: ModelConfig
        prompt_template: Sequence[LLMNodeChatModelMessage] | LLMNodeCompletionModelPromptTemplate
        prompt_config: PromptConfig = Field(default_factory=PromptConfig)
        memory: Optional[MemoryConfig] = None
        context: ContextConfig
        vision: VisionConfig = Field(default_factory=VisionConfig)

        @field_validator("prompt_config", mode="before")
        @classmethod
        def convert_none_prompt_config(cls, v: Any):
            if v is None:
                return PromptConfig()
            return v
    </core.workflow.nodes.llm.entities>

    <core.prompt.entities.advanced_prompt_entities>
    from typing import Literal, Optional

    from pydantic import BaseModel

    from core.model_runtime.entities.message_entities import PromptMessageRole


    class ChatModelMessage(BaseModel):
        """
        Chat Message.
        """

        text: str
        role: PromptMessageRole
        edition_type: Optional[Literal["basic", "jinja2"]] = None


    class CompletionModelPromptTemplate(BaseModel):
        """
        Completion Model Prompt Template.
        """

        text: str
        edition_type: Optional[Literal["basic", "jinja2"]] = None


    class MemoryConfig(BaseModel):
        """
        Memory Config.
        """

        class RolePrefix(BaseModel):
            """
            Role Prefix.
            """

            user: str
            assistant: str

        class WindowConfig(BaseModel):
            """
            Window Config.
            """

            enabled: bool
            size: Optional[int] = None

        role_prefix: Optional[RolePrefix] = None
        window: WindowConfig
        query_prompt_template: Optional[str] = None
    </core.prompt.entities.advanced_prompt_entities>
    <core.model_runtime.entities.message_entities>
    from abc import ABC
    from collections.abc import Sequence
    from enum import Enum, StrEnum
    from typing import Optional

    from pydantic import BaseModel, Field, computed_field, field_validator


    class PromptMessageRole(Enum):
        """
        Enum class for prompt message.
        """

        SYSTEM = "system"
        USER = "user"
        ASSISTANT = "assistant"
        TOOL = "tool"

        @classmethod
        def value_of(cls, value: str) -> "PromptMessageRole":
            """
            Get value of given mode.

            :param value: mode value
            :return: mode
            """
            for mode in cls:
                if mode.value == value:
                    return mode
            raise ValueError(f"invalid prompt message type value {value}")


    class PromptMessageTool(BaseModel):
        """
        Model class for prompt message tool.
        """

        name: str
        description: str
        parameters: dict


    class PromptMessageFunction(BaseModel):
        """
        Model class for prompt message function.
        """

        type: str = "function"
        function: PromptMessageTool


    class PromptMessageContentType(StrEnum):
        """
        Enum class for prompt message content type.
        """

        TEXT = "text"
        IMAGE = "image"
        AUDIO = "audio"
        VIDEO = "video"
        DOCUMENT = "document"


    class PromptMessageContent(BaseModel):
        """
        Model class for prompt message content.
        """

        type: PromptMessageContentType


    class TextPromptMessageContent(PromptMessageContent):
        """
        Model class for text prompt message content.
        """

        type: PromptMessageContentType = PromptMessageContentType.TEXT
        data: str


    class MultiModalPromptMessageContent(PromptMessageContent):
        """
        Model class for multi-modal prompt message content.
        """

        type: PromptMessageContentType
        format: str = Field(default=..., description="the format of multi-modal file")
        base64_data: str = Field(default="", description="the base64 data of multi-modal file")
        url: str = Field(default="", description="the url of multi-modal file")
        mime_type: str = Field(default=..., description="the mime type of multi-modal file")

        @computed_field(return_type=str)
        @property
        def data(self):
            return self.url or f"data:{self.mime_type};base64,{self.base64_data}"


    class VideoPromptMessageContent(MultiModalPromptMessageContent):
        type: PromptMessageContentType = PromptMessageContentType.VIDEO


    class AudioPromptMessageContent(MultiModalPromptMessageContent):
        type: PromptMessageContentType = PromptMessageContentType.AUDIO


    class ImagePromptMessageContent(MultiModalPromptMessageContent):
        """
        Model class for image prompt message content.
        """

        class DETAIL(StrEnum):
            LOW = "low"
            HIGH = "high"

        type: PromptMessageContentType = PromptMessageContentType.IMAGE
        detail: DETAIL = DETAIL.LOW


    class DocumentPromptMessageContent(MultiModalPromptMessageContent):
        type: PromptMessageContentType = PromptMessageContentType.DOCUMENT


    class PromptMessage(ABC, BaseModel):
        """
        Model class for prompt message.
        """

        role: PromptMessageRole
        content: Optional[str | Sequence[PromptMessageContent]] = None
        name: Optional[str] = None

        def is_empty(self) -> bool:
            """
            Check if prompt message is empty.

            :return: True if prompt message is empty, False otherwise
            """
            return not self.content


    class UserPromptMessage(PromptMessage):
        """
        Model class for user prompt message.
        """

        role: PromptMessageRole = PromptMessageRole.USER


    class AssistantPromptMessage(PromptMessage):
        """
        Model class for assistant prompt message.
        """

        class ToolCall(BaseModel):
            """
            Model class for assistant prompt message tool call.
            """

            class ToolCallFunction(BaseModel):
                """
                Model class for assistant prompt message tool call function.
                """

                name: str
                arguments: str

            id: str
            type: str
            function: ToolCallFunction

            @field_validator("id", mode="before")
            @classmethod
            def transform_id_to_str(cls, value) -> str:
                if not isinstance(value, str):
                    return str(value)
                else:
                    return value

        role: PromptMessageRole = PromptMessageRole.ASSISTANT
        tool_calls: list[ToolCall] = []

        def is_empty(self) -> bool:
            """
            Check if prompt message is empty.

            :return: True if prompt message is empty, False otherwise
            """
            if not super().is_empty() and not self.tool_calls:
                return False

            return True


    class SystemPromptMessage(PromptMessage):
        """
        Model class for system prompt message.
        """

        role: PromptMessageRole = PromptMessageRole.SYSTEM


    class ToolPromptMessage(PromptMessage):
        """
        Model class for tool prompt message.
        """

        role: PromptMessageRole = PromptMessageRole.TOOL
        tool_call_id: str

        def is_empty(self) -> bool:
            """
            Check if prompt message is empty.

            :return: True if prompt message is empty, False otherwise
            """
            if not super().is_empty() and not self.tool_call_id:
                return False

            return True
    </core.model_runtime.entities.message_entities>
    <core.workflow.entities.variable_entities>
    from collections.abc import Sequence

    from pydantic import BaseModel


    class VariableSelector(BaseModel):
        """
        Variable Selector.
        """

        variable: str
        value_selector: Sequence[str]
    </core.workflow.entities.variable_entities>
    <core.workflow.nodes.end.entities>
    from pydantic import BaseModel, Field

    from core.workflow.entities.variable_entities import VariableSelector
    from core.workflow.nodes.base import BaseNodeData


    class EndNodeData(BaseNodeData):
        """
        END Node Data.
        """

        outputs: list[VariableSelector]


    class EndStreamParam(BaseModel):
        """
        EndStreamParam entity
        """

        end_dependencies: dict[str, list[str]] = Field(
            ..., description="end dependencies (end node id -> dependent node ids)"
        )
        end_stream_variable_selector_mapping: dict[str, list[list[str]]] = Field(
            ..., description="end stream variable selector mapping (end node id -> stream variable selectors)"
        )
    </core.workflow.nodes.end.entities>
    <tests.unit_tests.core.workflow.nodes.llm>
    from collections.abc import Sequence
    from typing import Optional

    import pytest

    from configs import dify_config
    from core.app.entities.app_invoke_entities import InvokeFrom, ModelConfigWithCredentialsEntity
    from core.entities.provider_configuration import ProviderConfiguration, ProviderModelBundle
    from core.entities.provider_entities import CustomConfiguration, SystemConfiguration
    from core.file import File, FileTransferMethod, FileType
    from core.model_runtime.entities.common_entities import I18nObject
    from core.model_runtime.entities.message_entities import (
        AssistantPromptMessage,
        ImagePromptMessageContent,
        PromptMessage,
        PromptMessageRole,
        SystemPromptMessage,
        TextPromptMessageContent,
        UserPromptMessage,
    )
    from core.model_runtime.entities.model_entities import AIModelEntity, FetchFrom, ModelFeature, ModelType
    from core.model_runtime.model_providers.model_provider_factory import ModelProviderFactory
    from core.prompt.entities.advanced_prompt_entities import MemoryConfig
    from core.variables import ArrayAnySegment, ArrayFileSegment, NoneSegment, StringSegment
    from core.workflow.entities.variable_entities import VariableSelector
    from core.workflow.entities.variable_pool import VariablePool
    from core.workflow.graph_engine import Graph, GraphInitParams, GraphRuntimeState
    from core.workflow.nodes.answer import AnswerStreamGenerateRoute
    from core.workflow.nodes.end import EndStreamParam
    from core.workflow.nodes.llm.entities import (
        ContextConfig,
        LLMNodeChatModelMessage,
        LLMNodeData,
        ModelConfig,
        VisionConfig,
        VisionConfigOptions,
    )
    from core.workflow.nodes.llm.node import LLMNode
    from models.enums import UserFrom
    from models.provider import ProviderType
    from models.workflow import WorkflowType
    from tests.unit_tests.core.workflow.nodes.llm.test_scenarios import LLMNodeTestScenario


    class MockTokenBufferMemory:
        def __init__(self, history_messages=None):
            self.history_messages = history_messages or []

        def get_history_prompt_messages(
            self, max_token_limit: int = 2000, message_limit: Optional[int] = None
        ) -> Sequence[PromptMessage]:
            if message_limit is not None:
                return self.history_messages[-message_limit * 2 :]
            return self.history_messages


    @pytest.fixture
    def llm_node():
        data = LLMNodeData(
            title="Test LLM",
            model=ModelConfig(provider="openai", name="gpt-3.5-turbo", mode="chat", completion_params={}),
            prompt_template=[],
            memory=None,
            context=ContextConfig(enabled=False),
            vision=VisionConfig(
                enabled=True,
                configs=VisionConfigOptions(
                    variable_selector=["sys", "files"],
                    detail=ImagePromptMessageContent.DETAIL.HIGH,
                ),
            ),
        )
        variable_pool = VariablePool(
            system_variables={},
            user_inputs={},
        )
        node = LLMNode(
            id="1",
            config={
                "id": "1",
                "data": data.model_dump(),
            },
            graph_init_params=GraphInitParams(
                tenant_id="1",
                app_id="1",
                workflow_type=WorkflowType.WORKFLOW,
                workflow_id="1",
                graph_config={},
                user_id="1",
                user_from=UserFrom.ACCOUNT,
                invoke_from=InvokeFrom.SERVICE_API,
                call_depth=0,
            ),
            graph=Graph(
                root_node_id="1",
                answer_stream_generate_routes=AnswerStreamGenerateRoute(
                    answer_dependencies={},
                    answer_generate_route={},
                ),
                end_stream_param=EndStreamParam(
                    end_dependencies={},
                    end_stream_variable_selector_mapping={},
                ),
            ),
            graph_runtime_state=GraphRuntimeState(
                variable_pool=variable_pool,
                start_at=0,
            ),
        )
        return node


    @pytest.fixture
    def model_config():
        # Create actual provider and model type instances
        model_provider_factory = ModelProviderFactory()
        provider_instance = model_provider_factory.get_provider_instance("openai")
        model_type_instance = provider_instance.get_model_instance(ModelType.LLM)

        # Create a ProviderModelBundle
        provider_model_bundle = ProviderModelBundle(
            configuration=ProviderConfiguration(
                tenant_id="1",
                provider=provider_instance.get_provider_schema(),
                preferred_provider_type=ProviderType.CUSTOM,
                using_provider_type=ProviderType.CUSTOM,
                system_configuration=SystemConfiguration(enabled=False),
                custom_configuration=CustomConfiguration(provider=None),
                model_settings=[],
            ),
            provider_instance=provider_instance,
            model_type_instance=model_type_instance,
        )

        # Create and return a ModelConfigWithCredentialsEntity
        return ModelConfigWithCredentialsEntity(
            provider="openai",
            model="gpt-3.5-turbo",
            model_schema=AIModelEntity(
                model="gpt-3.5-turbo",
                label=I18nObject(en_US="GPT-3.5 Turbo"),
                model_type=ModelType.LLM,
                fetch_from=FetchFrom.CUSTOMIZABLE_MODEL,
                model_properties={},
            ),
            mode="chat",
            credentials={},
            parameters={},
            provider_model_bundle=provider_model_bundle,
        )


    def test_fetch_files_with_file_segment(llm_node):
        file = File(
            id="1",
            tenant_id="test",
            type=FileType.IMAGE,
            filename="test.jpg",
            transfer_method=FileTransferMethod.LOCAL_FILE,
            related_id="1",
            storage_key="",
        )
        llm_node.graph_runtime_state.variable_pool.add(["sys", "files"], file)

        result = llm_node._fetch_files(selector=["sys", "files"])
        assert result == [file]


    def test_fetch_files_with_array_file_segment(llm_node):
        files = [
            File(
                id="1",
                tenant_id="test",
                type=FileType.IMAGE,
                filename="test1.jpg",
                transfer_method=FileTransferMethod.LOCAL_FILE,
                related_id="1",
                storage_key="",
            ),
            File(
                id="2",
                tenant_id="test",
                type=FileType.IMAGE,
                filename="test2.jpg",
                transfer_method=FileTransferMethod.LOCAL_FILE,
                related_id="2",
                storage_key="",
            ),
        ]
        llm_node.graph_runtime_state.variable_pool.add(["sys", "files"], ArrayFileSegment(value=files))

        result = llm_node._fetch_files(selector=["sys", "files"])
        assert result == files


    def test_fetch_files_with_none_segment(llm_node):
        llm_node.graph_runtime_state.variable_pool.add(["sys", "files"], NoneSegment())

        result = llm_node._fetch_files(selector=["sys", "files"])
        assert result == []


    def test_fetch_files_with_array_any_segment(llm_node):
        llm_node.graph_runtime_state.variable_pool.add(["sys", "files"], ArrayAnySegment(value=[]))

        result = llm_node._fetch_files(selector=["sys", "files"])
        assert result == []


    def test_fetch_files_with_non_existent_variable(llm_node):
        result = llm_node._fetch_files(selector=["sys", "files"])
        assert result == []


    def test_fetch_prompt_messages__vison_disabled(faker, llm_node, model_config):
        prompt_template = []
        llm_node.node_data.prompt_template = prompt_template

        fake_vision_detail = faker.random_element(
            [ImagePromptMessageContent.DETAIL.HIGH, ImagePromptMessageContent.DETAIL.LOW]
        )
        fake_remote_url = faker.url()
        files = [
            File(
                id="1",
                tenant_id="test",
                type=FileType.IMAGE,
                filename="test1.jpg",
                transfer_method=FileTransferMethod.REMOTE_URL,
                remote_url=fake_remote_url,
                storage_key="",
            )
        ]

        fake_query = faker.sentence()

        prompt_messages, _ = llm_node._fetch_prompt_messages(
            sys_query=fake_query,
            sys_files=files,
            context=None,
            memory=None,
            model_config=model_config,
            prompt_template=prompt_template,
            memory_config=None,
            vision_enabled=False,
            vision_detail=fake_vision_detail,
            variable_pool=llm_node.graph_runtime_state.variable_pool,
            jinja2_variables=[],
        )

        assert prompt_messages == [UserPromptMessage(content=fake_query)]


    def test_fetch_prompt_messages__basic(faker, llm_node, model_config):
        # Setup dify config
        dify_config.MULTIMODAL_SEND_FORMAT = "url"

        # Generate fake values for prompt template
        fake_assistant_prompt = faker.sentence()
        fake_query = faker.sentence()
        fake_context = faker.sentence()
        fake_window_size = faker.random_int(min=1, max=3)
        fake_vision_detail = faker.random_element(
            [ImagePromptMessageContent.DETAIL.HIGH, ImagePromptMessageContent.DETAIL.LOW]
        )
        fake_remote_url = faker.url()

        # Setup mock memory with history messages
        mock_history = [
            UserPromptMessage(content=faker.sentence()),
            AssistantPromptMessage(content=faker.sentence()),
            UserPromptMessage(content=faker.sentence()),
            AssistantPromptMessage(content=faker.sentence()),
            UserPromptMessage(content=faker.sentence()),
            AssistantPromptMessage(content=faker.sentence()),
        ]

        # Setup memory configuration
        memory_config = MemoryConfig(
            role_prefix=MemoryConfig.RolePrefix(user="Human", assistant="Assistant"),
            window=MemoryConfig.WindowConfig(enabled=True, size=fake_window_size),
            query_prompt_template=None,
        )

        memory = MockTokenBufferMemory(history_messages=mock_history)

        # Test scenarios covering different file input combinations
        test_scenarios = [
            LLMNodeTestScenario(
                description="No files",
                sys_query=fake_query,
                sys_files=[],
                features=[],
                vision_enabled=False,
                vision_detail=None,
                window_size=fake_window_size,
                prompt_template=[
                    LLMNodeChatModelMessage(
                        text=fake_context,
                        role=PromptMessageRole.SYSTEM,
                        edition_type="basic",
                    ),
                    LLMNodeChatModelMessage(
                        text="{#context#}",
                        role=PromptMessageRole.USER,
                        edition_type="basic",
                    ),
                    LLMNodeChatModelMessage(
                        text=fake_assistant_prompt,
                        role=PromptMessageRole.ASSISTANT,
                        edition_type="basic",
                    ),
                ],
                expected_messages=[
                    SystemPromptMessage(content=fake_context),
                    UserPromptMessage(content=fake_context),
                    AssistantPromptMessage(content=fake_assistant_prompt),
                ]
                + mock_history[fake_window_size * -2 :]
                + [
                    UserPromptMessage(content=fake_query),
                ],
            ),
            LLMNodeTestScenario(
                description="User files",
                sys_query=fake_query,
                sys_files=[
                    File(
                        tenant_id="test",
                        type=FileType.IMAGE,
                        filename="test1.jpg",
                        transfer_method=FileTransferMethod.REMOTE_URL,
                        remote_url=fake_remote_url,
                        extension=".jpg",
                        mime_type="image/jpg",
                        storage_key="",
                    )
                ],
                vision_enabled=True,
                vision_detail=fake_vision_detail,
                features=[ModelFeature.VISION],
                window_size=fake_window_size,
                prompt_template=[
                    LLMNodeChatModelMessage(
                        text=fake_context,
                        role=PromptMessageRole.SYSTEM,
                        edition_type="basic",
                    ),
                    LLMNodeChatModelMessage(
                        text="{#context#}",
                        role=PromptMessageRole.USER,
                        edition_type="basic",
                    ),
                    LLMNodeChatModelMessage(
                        text=fake_assistant_prompt,
                        role=PromptMessageRole.ASSISTANT,
                        edition_type="basic",
                    ),
                ],
                expected_messages=[
                    SystemPromptMessage(content=fake_context),
                    UserPromptMessage(content=fake_context),
                    AssistantPromptMessage(content=fake_assistant_prompt),
                ]
                + mock_history[fake_window_size * -2 :]
                + [
                    UserPromptMessage(
                        content=[
                            TextPromptMessageContent(data=fake_query),
                            ImagePromptMessageContent(
                                url=fake_remote_url, mime_type="image/jpg", format="jpg", detail=fake_vision_detail
                            ),
                        ]
                    ),
                ],
            ),
            LLMNodeTestScenario(
                description="Prompt template with variable selector of File",
                sys_query=fake_query,
                sys_files=[],
                vision_enabled=False,
                vision_detail=fake_vision_detail,
                features=[ModelFeature.VISION],
                window_size=fake_window_size,
                prompt_template=[
                    LLMNodeChatModelMessage(
                        text="{{#input.image#}}",
                        role=PromptMessageRole.USER,
                        edition_type="basic",
                    ),
                ],
                expected_messages=[
                    UserPromptMessage(
                        content=[
                            ImagePromptMessageContent(
                                url=fake_remote_url, mime_type="image/jpg", format="jpg", detail=fake_vision_detail
                            ),
                        ]
                    ),
                ]
                + mock_history[fake_window_size * -2 :]
                + [UserPromptMessage(content=fake_query)],
                file_variables={
                    "input.image": File(
                        tenant_id="test",
                        type=FileType.IMAGE,
                        filename="test1.jpg",
                        transfer_method=FileTransferMethod.REMOTE_URL,
                        remote_url=fake_remote_url,
                        extension=".jpg",
                        mime_type="image/jpg",
                        storage_key="",
                    )
                },
            ),
        ]

        for scenario in test_scenarios:
            model_config.model_schema.features = scenario.features

            for k, v in scenario.file_variables.items():
                selector = k.split(".")
                llm_node.graph_runtime_state.variable_pool.add(selector, v)

            # Call the method under test
            prompt_messages, _ = llm_node._fetch_prompt_messages(
                sys_query=scenario.sys_query,
                sys_files=scenario.sys_files,
                context=fake_context,
                memory=memory,
                model_config=model_config,
                prompt_template=scenario.prompt_template,
                memory_config=memory_config,
                vision_enabled=scenario.vision_enabled,
                vision_detail=scenario.vision_detail,
                variable_pool=llm_node.graph_runtime_state.variable_pool,
                jinja2_variables=[],
            )

            # Verify the result
            assert len(prompt_messages) == len(scenario.expected_messages), f"Scenario failed: {scenario.description}"
            assert (
                prompt_messages == scenario.expected_messages
            ), f"Message content mismatch in scenario: {scenario.description}"


    def test_handle_list_messages_basic(llm_node):
        messages = [
            LLMNodeChatModelMessage(
                text="Hello, {#context#}",
                role=PromptMessageRole.USER,
                edition_type="basic",
            )
        ]
        context = "world"
        jinja2_variables = []
        variable_pool = llm_node.graph_runtime_state.variable_pool
        vision_detail_config = ImagePromptMessageContent.DETAIL.HIGH

        result = llm_node._handle_list_messages(
            messages=messages,
            context=context,
            jinja2_variables=jinja2_variables,
            variable_pool=variable_pool,
            vision_detail_config=vision_detail_config,
        )

        assert len(result) == 1
        assert isinstance(result[0], UserPromptMessage)
        assert result[0].content == [TextPromptMessageContent(data="Hello, world")]

    <tests.unit_tests.core.workflow.nodes.llm>
<source_code>


<instructions>
  <workflow_overview>
    提供された要件に基づいて、以下の要素を含むDifyワークフローファイル（YAML形式）を生成します：
    - ノード（開始、LLM、終了）で構成されるワークフロー
    - ノード間は順次実行されるよう接続（但し、指定された場合は並列実行も可能)
    - 入力から出力までの一連の処理を自動化
    </workflow_overview>    
  <required_structure>
    <app_info>
      - mode: workflowを指定
      - name: 目的を表す名前を設定
      - version: 0.1.5を使用
    </app_info>
  </required_structure>>


  <output_format>
    生成されるYAMLファイルは以下の形式に従ってください：
    ```yaml
    app:
      mode: workflow
      name: [ワークフロー名]
      version: 0.1.5
      description: ''
    kind: app
    version: 0.1.5
    workflow:
      graph:
        edges:
        # エッジ例
        - data:
            sourceType: [node type]
            targetType: [ターゲットノードタイプ]
          id: [source node id]-source-[target node ID]-target
          source: [start node id]
          sourceHandle: source
          target: [target node ID]
          targetHandle: target
        ## if-else エッジの例
        - data:
            sourceType: if-else
            targetType: [ターゲットノードタイプ]
          id: [source node id]-[caseid]-[target node ID]-target
          source: [source node id]
          sourceHandle: [case id]
          target: [target node ID]
          targetHandle: target
        - data:
            sourceType: if-else
            targetType: llm
          id: [source node id]-[caseid]-[target node ID]-target
          source: [source node id]
          sourceHandle: [case id]
          target: [target node ID]
          targetHandle: target
        # 以下 エッジのつながりを記述
        - data:
            sourceType: [node type]
            targetType: [ターゲットノードタイプ]
            ...


        nodes:
        # nodes例
          - data:
              type: [node type]
              title: [node name]
              # ノードごとの書式に従って 記述
          id: [node ID]
          # llm nodeの例
          - data:
            context:
              enabled: true
              variable_selector:
              - 'start-node-id'
              - input
            model:
              completion_params:
                temperature: 0.7
              mode: chat
              name: gpt-4o-mini
              provider: openai
            prompt_template:
              - id: [yml内でユニークになるID36文字以上]
                role: system
                text: '{{#start-node-id.input#}}の内容を要約して三十文字で出力'
            title: summary LLM
            type: llm
            variables: []
            vision:
              enabled: false
          id: '1736323130414'
        # end nodeの例
        - data:
            desc: ''
            outputs:
            - value_selector:
              - '1736323130414'
              - text
              variable: text
            selected: false
            title: 終了
            type: end
        id: '1736323188719'          
  
    ```
  </output_format>

  <output_notice>
    - ノードIDはユニークなID13文字
    - prompt_template のIDは 36文字、yml内で完全にユニークになるように設定
    - position,width,heigh,zoomなど位置関係は出力しない
    - openaiを使う時は gpt-4o-mini を使う
    - データの接続は context variable_selector を利用する
    - outputs は value_selector を利用する

  </output_notice>
</instructions>


<user_input>
入力した画像を元に俳句を読むワークフローを生成してください

```graph TD
    A[start: ワークフローの開始ノード] --> B[llm: llm]
    B[llm: llm] --> C[end: ワークフローの終了ノード]
```

</user_input>